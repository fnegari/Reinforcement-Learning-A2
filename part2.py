# -*- coding: utf-8 -*-
"""PART2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-Yprx2a3SrkimwkfnAOeNM35xhfon6z
"""

import numpy as np

# --- Gridworld Setup ---
grid_size = 5
terminal_states = [(2, 4), (4, 0), (2, 0)]  # Black terminal states

# Reward matrix (default -0.2)
rewards = np.full((grid_size, grid_size), -0.2)

# Transition model
transitions = {}
for i in range(grid_size):
    for j in range(grid_size):
        state = (i, j)
        transitions[state] = {}
        for action in ['up', 'down', 'left', 'right']:
            if state in terminal_states:
                # Terminal states stay in themselves with reward 0
                transitions[state][action] = ([state], 0)
            else:
                # Compute next state
                if action == 'up':
                    next_state = (max(i-1, 0), j)
                elif action == 'down':
                    next_state = (min(i+1, grid_size-1), j)
                elif action == 'left':
                    next_state = (i, max(j-1, 0))
                elif action == 'right':
                    next_state = (i, min(j+1, grid_size-1))

                # Rewards
                if next_state in terminal_states:
                    transitions[state][action] = ([next_state], 0)
                elif (i, j) == next_state:  # Hit wall
                    transitions[state][action] = ([next_state], -0.5)
                else:
                    transitions[state][action] = ([next_state], rewards[next_state[0], next_state[1]])

def is_terminal(state):
    return state in terminal_states

def get_next_state(state, action):
    i, j = state
    if action == 'up':
        return (max(i-1, 0), j)
    elif action == 'down':
        return (min(i+1, grid_size-1), j)
    elif action == 'left':
        return (i, max(j-1, 0))
    elif action == 'right':
        return (i, min(j+1, grid_size-1))

print("Transitions[(0,0)]:", transitions[(0,0)])

import random

# Initialize value function
V = np.zeros((grid_size, grid_size))
gamma = 0.95
episodes = 10000

policy = lambda s: random.choice(['up', 'down', 'left', 'right'])

# Monte Carlo method with exploring starts
def monte_carlo_es(policy, V, rewards, gamma, episodes):
    for episode in range(episodes):
        state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))
        while is_terminal(state):
            state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))
        episode = []
        while not is_terminal(state):
            action = random.choice(['up', 'down', 'left', 'right'])
            # Get the list of possible next states and rewards
            possible_transitions = transitions[state][action]
            # Handle cases where transitions are defined differently
            if isinstance(possible_transitions[0], tuple):  # If the first element is a tuple (state, reward)
                next_state, reward = possible_transitions[0]
            else:  # If the first element is a list of states and the second is the reward
                next_state = random.choice(possible_transitions[0])  # Randomly choose from possible next states
                reward = possible_transitions[1]
            episode.append((state, action, reward))
            state = next_state
        G = 0
        for state, action, reward in reversed(episode):
            G = gamma * G + reward
            # Update value function
            V[state] += (G - V[state]) / episodes
    return V

V = monte_carlo_es(policy, V, rewards, gamma, episodes)
print("Value Function (Monte Carlo Exploring Starts):")
print(V)

def policy(state):
    return random.choice(['up', 'down', 'left', 'right'])  # Ensure policy always returns a valid action

# Monte Carlo method with ϵ-soft policy
def monte_carlo_epsilon_soft(policy, V, rewards, gamma, epsilon, episodes):
    for episode in range(episodes):
        state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))
        while is_terminal(state):
            state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))
        episode = []
        while not is_terminal(state):
            if random.uniform(0, 1) < epsilon:
                action = random.choice(['up', 'down', 'left', 'right'])
            else:
                # Call the policy function to get the action
                action = policy(state)
            # Access transitions and handle different formats
            transition = transitions[state][action]
            if isinstance(transition[0], tuple):
                next_state, reward = transition[0]
            else:
                next_state = random.choice(transition[0])
                reward = transition[1]
            episode.append((state, action, reward))
            state = next_state
        G = 0
        for state, action, reward in reversed(episode):
            G = gamma * G + reward
            # Update value function
            V[state] += (G - V[state]) / episodes
    return V

epsilon = 0.1

V = np.zeros((grid_size, grid_size))
V = monte_carlo_epsilon_soft(policy, V, rewards, gamma, epsilon, episodes)
print("Value Function (Monte Carlo ϵ-soft):")
print(V)

# Behaviour Policy with Equiprobable Moves
def behaviour_policy(grid_size, episodes, gamma=0.95):
    policy = np.ones((grid_size, grid_size, 4)) / 4  # Equiprobable policy
    Q = np.zeros((grid_size, grid_size, 4))
    C = np.zeros((grid_size, grid_size, 4))

    actions = ['up', 'down', 'left', 'right']
    action_to_idx = {action: idx for idx, action in enumerate(actions)}

    def get_action(state):
        return random.choice(actions)

    for episode in range(episodes):
        state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))
        while is_terminal(state):
            state = (random.randint(0, grid_size-1), random.randint(0, grid_size-1))

        episode = []
        while not is_terminal(state):
            action = random.choice(['up', 'down', 'left', 'right'])
            # Get the list of possible next states and rewards
            possible_transitions = transitions[state][action]
            # Handle cases where transitions are defined differently
            if isinstance(possible_transitions[0], tuple):  # If the first element is a tuple (state, reward)
                next_state, reward = possible_transitions[0]
            else:  # If the first element is a list of states and the second is the reward
                next_state = random.choice(possible_transitions[0])  # Randomly choose from possible next states
                reward = possible_transitions[1]
            episode.append((state, action, reward))
            state = next_state

        G = 0
        W = 1
        for state, action, reward in reversed(episode):
            G = gamma * G + reward
            action_idx = action_to_idx[action]
            C[state][action_idx] += W
            Q[state][action_idx] += (W / C[state][action_idx]) * (G - Q[state][action_idx])
            policy[state] = np.eye(4)[np.argmax(Q[state])]
            if action != np.argmax(policy[state]):
                break
            W /= 0.25  # Since behaviour policy is equiprobable
    return policy

policy = behaviour_policy(grid_size, episodes, gamma)
print("Optimal Policy (Behaviour Policy):")
print(policy)

# Policy Iteration with Permuted Squares
def policy_iteration_permuted(grid_size, iterations, gamma=0.95, permute_prob=0.1):
    policy = np.ones((grid_size, grid_size, 4)) / 4  # Equiprobable policy initially
    V = np.zeros((grid_size, grid_size))

    actions = ['up', 'down', 'left', 'right']
    action_to_idx = {action: idx for idx, action in enumerate(actions)}

    def get_next_state(state, action):
        i, j = state
        if action == 'up':
            return (max(i-1, 0), j)
        elif action == 'down':
            return (min(i+1, grid_size-1), j)
        elif action == 'left':
            return (i, max(j-1, 0))
        elif action == 'right':
            return (i, min(j+1, grid_size-1))

    for iteration in range(iterations):
        # Policy Evaluation
        while True:
            delta = 0
            for i in range(grid_size):
                for j in range(grid_size):
                    state = (i, j)
                    if is_terminal(state):
                        continue
                    v = V[state]
                    new_v = sum([policy[state][action_to_idx[action]] *
                                 (rewards[state] + gamma * V[get_next_state(state, action)])
                                 for action in actions])
                    V[state] = new_v
                    delta = max(delta, abs(v - new_v))
            if delta < 1e-6:
                break

        # Policy Improvement
        policy_stable = True
        for i in range(grid_size):
            for j in range(grid_size):
                state = (i, j)
                if is_terminal(state):
                    continue
                old_action = np.argmax(policy[state])
                action_values = np.zeros(4)
                for action in actions:
                    next_state = get_next_state(state, action)
                    action_values[action_to_idx[action]] = rewards[state] + gamma * V[next_state]
                best_action = np.argmax(action_values)
                policy[state] = np.eye(4)[best_action]
                if old_action != best_action:
                    policy_stable = False

        if policy_stable:
            break

        # Permute Green and Blue squares with probability 0.1
        if np.random.rand() < permute_prob:
            # Swap locations of green and blue squares
            rewards[(0, 1)], rewards[(0, 4)] = rewards[(0, 4)], rewards[(0, 1)]

    return policy

policy = policy_iteration_permuted(grid_size, 100, gamma)
print("Optimal Policy (Policy Iteration with Permuted Squares):")
print(policy)